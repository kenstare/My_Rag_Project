{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b90320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "104f618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7bcac",
   "metadata": {},
   "source": [
    "#### Documents collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13885487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 pages from PDFs\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for pdf_path in glob.glob(\"documents/*.pdf\"):  # adjust folder path\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83da7f4",
   "metadata": {},
   "source": [
    "#### Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b47191d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 7 documents into 33 chunks\n",
      "\n",
      "Chunk 1: AI & ML Projects\n",
      "AI & ML Projects\n",
      "AI & ML Projects (Combined)\n",
      "AI & ML Projects (updated)\n",
      "AI & ML Projects (Updated)\n",
      "Project 1: World Population Forecast\n",
      "- Objective: Forecast world population trends using demographic and\n",
      "socio-economic data.\n",
      "\n",
      "Chunk 2: - Methods: Time series modeling, ARIMA, Prophet, LSTM/Transformer sequence\n",
      "models.\n",
      "- Data sources: UN population estimates, World Bank indicators.\n",
      "Project 2: Predicting Food Prices in Nigeria\n",
      "- Objective: Build models to predict food price movements in Nigerian markets.\n",
      "\n",
      "Chunk 3: - Methods: Regression models, tree-based models (XGBoost/LightGBM), temporal\n",
      "models.\n",
      "- Features: Supply indicators, weather, inflation, transport costs, seasonality.\n",
      "Project 3: Indian Credit Card / Car Price Predicting\n",
      "\n",
      "Chunk 4: - Objective: Predict credit card default risk and car price/value estimation in Indian\n",
      "markets.\n",
      "- Methods: Classification for credit risk; regression for car price prediction using\n",
      "features like year, mileage, make/model, region.\n",
      "\n",
      "Chunk 5: - Data sources: Public datasets, scraped market listings, financial records.\n",
      "Cloud Computing (Added to projects and personal info)\n",
      "- Overview: Cloud resources and patterns useful for deploying ML workloads and\n",
      "data pipelines.\n",
      "- Key services to know and practice:\n",
      "\n",
      "Chunk 6: - Amazon EC2: Launch and manage virtual server instances for model training and\n",
      "inference.\n",
      "- Amazon S3: Store datasets, model artifacts, and logs with lifecycle rules and\n",
      "versioning.\n",
      "- Amazon VPC: Create isolated networks, subnets, routing tables, security groups\n",
      "for secure deployments.\n",
      "\n",
      "Chunk 7: - IAM: Manage users, roles, and policies for least-privilege access.\n",
      "- Auto Scaling & Load Balancing: Scale model serving and APIs for availability and\n",
      "cost-efficiency.\n",
      "- RDS / DynamoDB: Managed databases for metadata and application state.\n",
      "\n",
      "Chunk 8: - AWS Lambda & Step Functions: Serverless orchestration for lightweight tasks\n",
      "and ETL.\n",
      "- Suggested hands-on tasks:\n",
      "1. Create an S3 bucket, upload sample datasets, enable versioning.\n",
      "\n",
      "Chunk 9: 2. Launch an EC2 instance, install Python + ML stack, run a small training job.\n",
      "3. Set up a VPC with public and private subnets, attach a security group permitting\n",
      "only necessary ports.\n",
      "4. Create IAM roles for EC2 and for a CI/CD pipeline with least privilege.\n",
      "\n",
      "Chunk 10: 5. Deploy a simple model behind an Application Load Balancer with an Auto\n",
      "Scaling Group.\n",
      "- Notes:\n",
      "- Follow best practices for credentials: use IAM roles, avoid embedding secrets,\n",
      "rotate keys.\n",
      "- Consider cost management: use spot instances for training, lifecycle policies for\n",
      "S3.\n",
      "\n",
      "Chunk 11: S3.\n",
      "Cloud_Computing_Info\n",
      "Cloud Computing — Practical Guide for Personal Info\n",
      "This document provides hands-on tasks and brief explanations to add cloud\n",
      "computing skills to your profile.\n",
      "Core Concepts\n",
      "- EC2: Virtual machines. Use for training small models, experimenting, and running\n",
      "containers.\n",
      "\n",
      "Chunk 12: containers.\n",
      "- S3: Object storage. Store raw data, preprocessed datasets, model checkpoints,\n",
      "and logs.\n",
      "- VPC: Virtual Private Cloud to isolate resources, create private subnets for\n",
      "databases, public subnets for load balancers.\n",
      "\n",
      "Chunk 13: - IAM: Identity and Access Management for users, groups, roles, and policies.\n",
      "- Autoscaling & ELB: Keep services available and scale according to load.\n",
      "Getting started (practical steps)\n",
      "1. Create an S3 bucket; upload a sample CSV dataset; enable versioning and set a\n",
      "\n",
      "Chunk 14: lifecycle rule to archive older data.\n",
      "2. Launch an EC2 instance (Ubuntu 22.04), SSH in, install Python, pip, and\n",
      "common ML libraries (numpy, pandas, scikit-learn, torch/tensorflow).\n",
      "3. Configure an IAM role for the EC2 instance with S3 read/write permissions\n",
      "\n",
      "Chunk 15: (avoid storing AWS keys on the instance).\n",
      "4. Design a VPC: create subnets, route tables, an internet gateway for public\n",
      "subnet; put databases in private subnet.\n",
      "5. Deploy a small Flask/FastAPI model server on EC2 and put it behind an\n",
      "Application Load Balancer. Configure an Auto Scaling Group.\n",
      "\n",
      "Chunk 16: 6. (Optional) Use AWS Lambda + Step Functions for scheduled ETL jobs that\n",
      "move data from S3 to a database.\n",
      "Security & Cost\n",
      "- Use Security Groups to restrict inbound traffic.\n",
      "- Use IAM roles with least privilege.\n",
      "- Monitor costs; use AWS Cost Explorer and set budgets/alerts.\n",
      "\n",
      "Chunk 17: - Use spot instances for large training jobs to reduce costs.\n",
      "\n",
      "Chunk 18: Personal Biography\n",
      "My name is Kehinde Akindele, an AI Engineer with a strong background in mathematics, data science, and \n",
      "applied machine learning. I specialize in building intelligent systems that combine predictive modeling, \n",
      "natural language processing, and data-driven decision -making.\n",
      "\n",
      "Chunk 19: I hold a Bachelor’s degree in Applied Mathematics and a Master’s degree in Artificial Intelligence. Over \n",
      "the past five years, I have worked on real -world machine learning projects across finance, e -commerce, \n",
      "and education sectors.\n",
      "\n",
      "Chunk 20: My interests include retrieval -augmented generation (RAG), large language models, time -series \n",
      "forecasting, and AI system deployment. I enjoy teaching and mentoring students in mathematics, \n",
      "Python programming, and machine learning concepts.\n",
      "\n",
      "Chunk 21: Outside of work, I actively contribute to open -source projects, write technical blog posts, and explore \n",
      "ways AI can be responsibly deployed in emerging markets.\n",
      "\n",
      "Chunk 22: Kehinde Akindele\n",
      "AI Engineer & Data Scientist\n",
      "Professional Summary\n",
      "AI Engineer with a strong background in mathematics, data science, and applied machine\n",
      "learning. Specializes in building intelligent systems that combine predictive modeling,\n",
      "\n",
      "Chunk 23: natural language processing, and data-driven decision-making. Experienced in designing,\n",
      "training, and deploying machine learning models.\n",
      "Education\n",
      "M.Sc. Artificial Intelligence; B.Sc. Applied Mathematics\n",
      "Technical Skills\n",
      "- Programming: Python, SQL, JavaScript\n",
      "\n",
      "Chunk 24: - Machine Learning: Regression, Classification, Time-Series Forecasting\n",
      "- Deep Learning: Neural Networks, Transformers\n",
      "- NLP: Text Embeddings, Vector Databases, RAG Systems\n",
      "- Tools: LangChain, Chroma, PyTorch, TensorFlow, Docker, Git\n",
      "Experience\n",
      "\n",
      "Chunk 25: Experience\n",
      "- Built predictive models for sales forecasting in e-commerce\n",
      "- Developed AI-powered chatbots for document-based question answering\n",
      "- Designed REST APIs for ML model deployment using Flask/FastAPI\n",
      "Research Interests\n",
      "\n",
      "Chunk 26: Research Interests\n",
      "Retrieval-Augmented Generation (RAG), large language models, time-series forecasting,\n",
      "and AI system deployment. Interested in ethical AI and model interpretability.\n",
      "Teaching & Mentorship\n",
      "- Algebra, Calculus, and Probability\n",
      "- Python programming for beginners\n",
      "\n",
      "Chunk 27: - Machine learning fundamentals\n",
      "- Data analysis using Python and Excel\n",
      "Email: akindekekehinde250@gmail.com | GitHub:\n",
      "https://github.com/kenstare?tab=repositories\n",
      "\n",
      "Chunk 28: My  research  interests  lie  in  building  intelligent  systems  that  combine  information  retrieval  and  language  understanding.  I  am  particularly  interested  in  Retrieval-  Augmented  Generation  (RAG)  systems  and  their  applications  in  education  and  finance.   My  long-term  goal\n",
      "\n",
      "Chunk 29: My  long-term  goal  is  to  become  a  lead  AI  engineer,  designing  scalable  AI  platforms  that  assists  decision-making.  I  aim  to  contribute  to  AI  research  while  also  building  practical  tools  that  solve  real-world  problems.   I  am  passionate  about  ethical  AI,  model\n",
      "\n",
      "Chunk 30: AI,  model  interpretability,  and  making  AI  accessible  to  non-technical  users.\n",
      "\n",
      "Chunk 31: I  have  extensive  experience  teaching  mathematics  and  programming  to  high  school  and  undergraduate  students.  I  conduct  online  classes  using  Google  Meet  and  Zoom,  focusing  on  clarity,  problem-solving,  and  real-world  applications.   Subjects  taught  include:  -  Algebra,\n",
      "\n",
      "Chunk 32: -  Algebra,  Calculus,  and  Probability  -  Python  programming  for  beginners  -  Machine  learning  fundamentals  -  Data  analysis  using  Python  and  Excel   I  also  mentor  students  working  on  AI  and  data  science  projects,  guiding  them  through  problem  formulation,  model\n",
      "\n",
      "Chunk 33: model  selection,  and  evaluation  techniques.\n"
     ]
    }
   ],
   "source": [
    "# Create splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf97faa",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d24ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n",
      "First 5 values: [0.0006281227106228471, 0.02569717727601528, 0.007161187008023262, 0.03336399793624878, -0.031968604773283005]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48504251",
   "metadata": {},
   "source": [
    "#### Vector Store\n",
    "# Create vector store from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8151c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    collection_name=\"my_info_collection\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5339b8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'moddate': '2025-12-14T18:59:52+01:00', 'keywords': '', 'title': '(anonymous)', 'page_label': '1', 'trapped': '/False', 'total_pages': 1, 'creator': '(unspecified)', 'subject': '(unspecified)', 'source': 'documents\\\\Professional Resume.pdf', 'page': 0, 'author': '(anonymous)', 'creationdate': '2025-12-14T18:59:52+01:00', 'producer': 'ReportLab PDF Library - (opensource)'}, page_content='natural language processing, and data-driven decision-making. Experienced in designing,\\ntraining, and deploying machine learning models.\\nEducation\\nM.Sc. Artificial Intelligence; B.Sc. Applied Mathematics\\nTechnical Skills\\n- Programming: Python, SQL, JavaScript'),\n",
       " Document(metadata={'trapped': '/False', 'source': 'documents\\\\Professional Resume.pdf', 'page': 0, 'creationdate': '2025-12-14T18:59:52+01:00', 'subject': '(unspecified)', 'author': '(anonymous)', 'moddate': '2025-12-14T18:59:52+01:00', 'total_pages': 1, 'producer': 'ReportLab PDF Library - (opensource)', 'keywords': '', 'page_label': '1', 'creator': '(unspecified)', 'title': '(anonymous)'}, page_content='natural language processing, and data-driven decision-making. Experienced in designing,\\ntraining, and deploying machine learning models.\\nEducation\\nM.Sc. Artificial Intelligence; B.Sc. Applied Mathematics\\nTechnical Skills\\n- Programming: Python, SQL, JavaScript'),\n",
       " Document(metadata={'creator': '(unspecified)', 'page_label': '1', 'title': '(anonymous)', 'trapped': '/False', 'moddate': '2025-12-14T18:54:21+01:00', 'subject': '(unspecified)', 'source': 'documents\\\\Professional Resume.pdf', 'keywords': '', 'author': '(anonymous)', 'total_pages': 1, 'producer': 'ReportLab PDF Library - (opensource)', 'creationdate': '2025-12-14T18:54:21+01:00', 'page': 0}, page_content='natural language processing, and data-driven decision-making. Experienced in designing,\\ntraining, and deploying machine learning models.\\nEducation\\nM.Sc. Artificial Intelligence; B.Sc. Applied Mathematics\\nTechnical Skills\\n- Programming: Python, SQL, JavaScript')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test retriver\n",
    "query = \"Technical skills\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8510f",
   "metadata": {},
   "source": [
    "#### Conversational Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1955fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant answering questions about Kehinde Akindele using the provided documents.\n",
    "\n",
    "Use ONLY the context below to answer the question.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in clear sentences.\n",
    "At the end, list the sources you used as bullet points.\n",
    "\"\"\")\n",
    "\n",
    "# format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('source', 'unknown')}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# RAG chain Using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "211787ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kehinde Akindele has worked on AI projects that involve building intelligent systems combining predictive modeling. Specific details about the AI projects are not provided in the context.\n",
      "\n",
      "Sources:\n",
      "- documents\\Professional Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"What AI projects has Kehinde worked on?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062785b",
   "metadata": {},
   "source": [
    "#### Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12079bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for chat histories\n",
    "chat_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# Create conversational prompt\n",
    "conv_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant answering questions about Kehinde Akindele using the provided documents. Use ONLY the context below to answer the question. If the answer is not in the context, say I don't know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"system\", \"Answer in clear sentences. At the end, list the sources you used as bullet points.\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Build base chain\n",
    "conv_chain_base = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        question=lambda x: x[\"question\"],\n",
    "        chat_history=lambda x: x.get(\"chat_history\", [])\n",
    "    )\n",
    "    | conv_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with message history\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    conv_chain_base,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdecec4",
   "metadata": {},
   "source": [
    "\n",
    "**Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec58bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      " Kehinde Akindele has worked on projects involving building intelligent systems that combine predictive modeling.\n",
      "\n",
      "Response 2:\n",
      " Kehinde Akindele has worked on projects involving RAG systems in building predictive models for sales forecasting in e-commerce and developing AI-powered chatbots for document-based question answering. \n",
      "\n",
      "Sources:\n",
      "- Professional Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "response = conv_chain.invoke(\n",
    "    {\"question\": \"What projects has Kehinde worked on?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(\"Response 1:\\n\", response)\n",
    "\n",
    "# Follow-up question\n",
    "response2 = conv_chain.invoke(\n",
    "    {\"question\": \"Which of those involve RAG systems?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nResponse 2:\\n\", response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
